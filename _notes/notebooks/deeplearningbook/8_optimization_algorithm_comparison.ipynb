{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8_optimization algorithm_comparison.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mandarup/mandarup.github.io/blob/master/_notes/notebooks/deeplearningbook/8_optimization_algorithm_comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnPg7-OqexEJ",
        "colab_type": "text"
      },
      "source": [
        "# Key Differences in Optimization Algorithms for Deep Learning\n",
        "\n",
        "\n",
        "Except for Algorithms using Nesterov momentum, gradient is calculated as:\n",
        "> $g \\leftarrow  1/m  \\nabla_\\theta \\sum_i{L(f(x^{(i)}, \\theta), y^{(i)})}$\n",
        "\n",
        "When using Nesterov momentum, first interim $\\theta$ is computed as\n",
        "> $\\tilde\\theta \\leftarrow \\theta + \\alpha v$ \\\\\n",
        "\n",
        "> $g \\leftarrow  1/m  \\nabla_\\tilde\\theta \\sum_i{L(f(x^{(i)}, \\tilde\\theta), y^{(i)})}$\n",
        "\n",
        "\n",
        "> Algorithm | Require |  Parameter Update | AdditionalNotes\n",
        ">  --- | --- | --- | ---\n",
        "> SGD | $\\epsilon$  |  $$\\theta \\leftarrow \\theta - \\epsilon  g$$ | \n",
        "> Momentum |  $\\epsilon, \\alpha$ <br> init variables $\\theta, v$ | $$v \\leftarrow \\alpha v - \\epsilon g$$  $$\\theta \\leftarrow \\theta + v$$| \n",
        "> Nesterov Momentum | $\\epsilon, \\alpha$ <br> init variables $\\theta, v$ | $$\\tilde\\theta \\leftarrow \\theta + \\alpha v$$  $$g \\leftarrow  1/m  \\nabla_\\tilde\\theta \\sum_i{L(f(x^{(i)}, \\tilde\\theta), y^{(i)})}$$  $$v \\leftarrow \\alpha v - \\epsilon g$$  $$\\theta \\leftarrow \\theta + v$$|\n",
        "> Adagrad |  $\\epsilon$ <br> init variables $\\theta; r=0$ <br>$ \\delta =10^{-7}$  |$ r \\leftarrow r + g \\odot g$ <br> $\\Delta \\theta \\leftarrow - \\frac{\\epsilon}{\\sqrt{\\delta} + r} \\odot g $|\n",
        "> RMSProp |   $\\epsilon, \\rho$ <br> init variables $\\theta; r=0$ <br>$ \\delta =10^{-6}$|  $ r \\leftarrow \\rho r + (1-\\rho)g \\odot g$ <br> $\\Delta \\theta \\leftarrow - \\frac{\\epsilon}{\\sqrt{\\delta + r}} \\odot g $ |\n",
        "> RMSProp with <br>Nesterov Momentum |  $\\epsilon, \\rho,  \\alpha$ <br> init variables $\\theta; r=0; v; $ <br>$ \\delta =10^{-6}$ | $\\tilde \\theta \\leftarrow \\theta + \\alpha v$ <br>$g \\leftarrow  1/m  \\nabla_\\tilde\\theta \\sum_i{L(f(x^{(i)}, \\tilde\\theta), y^{(i)})}$ <br>  $ r \\leftarrow \\rho r + (1-\\rho)g \\odot g$ <br> $ v \\leftarrow \\alpha v - \\frac{\\epsilon}{\\sqrt{r}} \\odot g$ <br> $ \\theta \\leftarrow \\theta + v$|  interim update <br> \n",
        "> Adam | | $t \\leftarrow t + 1$ <br> $s  \\leftarrow \\rho_1 s + (1- \\rho_1) g $ <br> $r \\leftarrow \\rho_2 r + (1 - \\rho_2 ) g \\odot g$ <br> $\\hat s \\leftarrow \\frac {s} {1 - \\rho^t_1}$ <br> $\\hat r \\leftarrow \\frac {r} {1 - \\rho^t_2}$ <br> $\\Delta \\theta \\leftarrow  - \\epsilon \\frac{\\hat s}{\\sqrt{\\hat r } + \\delta}$ <br> $\\theta \\leftarrow \\theta + \\Delta \\theta$| increment time <br> first moment <br> second moment <br> first moment bias corrections  <br> second moment bias correction <br>elementwise ops\n",
        "\n",
        "\n",
        "\n",
        "* there is no consensus on choice of algorithm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcLJgyCRejmG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}